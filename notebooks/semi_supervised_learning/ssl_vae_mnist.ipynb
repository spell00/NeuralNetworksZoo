{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why semi-supervised learning?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Remark: you should not run this notebook if you don't have an NVIDIA GPU. Also make sure that pyTorch is able to use it. It will still run, but it might take too long. Note that good results can be obtained on a CPU if the dataset is small or the classification task is relatively easy With MNISt, it is possible to get acceptable results in a few epochs, even as few as < 10. This example is not like that, it takes a few hundred epochs to get good results (used to be state-of-the-art for ssl, to be verified for present code). \n",
    "\n",
    "All articles and the implementations I have encountered until now use binarized MNIST and many people in blogs (stackoverflow, reddit,  wordpress...) and some said VAE was not performing well on real-valued MNIST. \n",
    "    \n",
    "One problem is that cross entropy loss apprears to be people's go-to loss. Cross entropy is usually the best choice when dealing with a classification task; the target is in the one-hot form, which makes cross-entropy converge must faster. Cross-entropy is better adapted when all values are probabilities and thus the output vector sum to 1 (softmax layer). MSE would have more difficulty to converge when the target is just 0s and 1s. MSE have more difficulty to converge when the correct values are the most extrem values. When using a softmax, the target is a one-hot vector, so the optimal values are always going to be extrem values. \n",
    "    \n",
    "In contrast, in an auto-encoder, the outputs are not probabilities and the last layer is not a softmax, so it doesn't have to add to anything. The best values could take any value between the maximum and the minimum values, which could be [-infinity ,+infinite]. One strategy to reduce the range of possible values is to apply a sigmoid function to make it in the [0, 1] range.\n",
    "\n",
    "Instead of sigmoid, I used tanh. Because the data is standardized (note: should try without), the values can be in the range [-infinity, +infinity]. When I used sigmoid, the images were good, but with no white, just tones of grey. This was solved with tanh; the images were in the [-1,1] range, so tanh solved the problem. The images could be tranformed to be [0,1], but it should be equivalent. \n",
    "\n",
    "NOTE: IN the gene expression analysis, I also used tanh, because the raw data should be [0, +infinity], as they are mRNA \"counts\" (negative amount of an RNA makes no sense). Sigmoid would not be appropriate, as there are no negative values; so the minimum value would be 0.5 and maximum 1.0, which is not practical (why though? ) and not intuitive in the interpretaition of the data. Tanh, on the other hand, would make the data of range [0, +infinity] become [0, 1] (tanh is [-1, 1], but no data will fall in [-1, 0], as there are no negative values)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "For the reasons mentionned above, in the M2 model (SSL), the MSE for reconstruction makes more sense, thus the images get better reconstruction much faster, but the classifier has to use MSE also. \n",
    "\n",
    "As a solution, we could use a relaxation of the one-hot vector into a \"softer\" representation of the categorical distribution using a gumbel-softmax. The new vector will then be similar to a one-hot vector, but all the other values will be a little higher than 0 and the 1 will be less than one. I do not know well how it works, but the classes that have more chance to be confused with the actual class should receive more probability in the relaxation than those that are (almost) never confused. \n",
    "For example, in MNIST, 8 and 9 have more chances of being confused than 8 and 4 (intuitively, I do not know if it is a fact).\n",
    "\n",
    "Note 2:\n",
    "I add an extra class, which do not correspond with any of the classes in the dataset. This class represent anything else (in the code I call it the class \"N/A\") than the labels proposed by the dataset. If labels come from a limited set, it should not make any difference. If we take MNIST, for example, only digits in {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} are possible. Having an extra class, or any additional number of classes, should not affect the classification accuracy, as it should be simply ignored. \n",
    "\n",
    "The advantage of this COULD be:\n",
    "\n",
    "1- After a deep learning model is learned, it SHOULD never be a disadvantage to have an extra class, even if it is a useless class (it would always be an advantage or not affect the classification). *** Not having it might not improve the accuracy, but it could still be a good idea. \n",
    "\n",
    "For example, if designing a self-driving car AI, let's say the \n",
    "car has to take a the decision to move forward or stay put, \n",
    "based on what it has learned. Here is one way to take the \n",
    "decision with a binary classifier (1 output node*):\n",
    "    1- P(Safe) >= 0.9999:\n",
    "        Safe to move forward -> action taken: Move forward\n",
    "    2- P(Safe) < 0.9999:\n",
    "        Not safe enough to move forward -> action taken: Stay put\n",
    "\n",
    "* We don't want 2 outputs, because we only want to predict if it is safe or not; we probably want self-driving cars to be more cautious than not enough (at least I do), so we could model how safe a situation is and decide to go if the probability of the situation being safe if more than 99.99% (it could never be 100%, otherwise the car would never move). \n",
    "\n",
    "The alternative decision making process if an additional class, representing the uncertain decision, would be :\n",
    "    1- Max(Output nodes) == Safe -> action taken: move forward\n",
    "    2- Max(Output nodes) == Not Safe -> action taken: Stay put\n",
    "    3- Max(Output nodes) == Not sure -> action taken: ***Stay put\n",
    "\n",
    "*** In this context of prudence, the decision when the \"not sure\" node is the most probable would be to do nothing, but other contexts might favour to take action in the case of uncertainty. \n",
    "    For example, in the context of a an alarm system which monitors \n",
    "    sounds to decide if it detected an intrusion, one might want it \n",
    "    to be more sensitive than not enough\n",
    "    \n",
    "Conclusions:\n",
    "    1- If a single node is used, the network learns to evaluate the \n",
    "       probabiltity of a situation to be dangerous; the threshold \n",
    "       for what is \"safe enough\" is up the user (or even worst, \n",
    "       only to the programmer or someone who knows how to change\n",
    "       it).\n",
    "       \n",
    "       The network will only learn the concept of \"how probable is \n",
    "       it that it is a safe situation\" (might not necessarily mean \n",
    "       \"how safe is it to advance forward?\"). \n",
    "       \n",
    "       We don't know if a situation is unsafe: if p=P(Safe), then \n",
    "       (1-p) means \"how probable is it not safe\", which is not the \n",
    "       same as \"How unsafe is it?\". It might make more sense for \n",
    "       later to rephrase this as \"how dangerous is it?\". It learned \n",
    "       the concept of 'Safety', but also the concept of 'Danger'.\n",
    "       \n",
    "       \n",
    "    2- If 2 nodes are used, the network will learn:\n",
    "        1. Node P(Safe): how probable is it safe to \n",
    "                         advance forward?\n",
    "        2. Node P(not Safe): how probable is it not safe to advance \n",
    "                      forward?\n",
    "        \n",
    "        It is the same as the 1-node model. If the first node is p, \n",
    "        then the second node is always (1-p).\n",
    "        \n",
    "    3- If 3 nodes are used, the network will learn:    \n",
    "        1. Node     P(Safe): How probable is it safe to \n",
    "                             advance forward?\n",
    "        2. Node P(not Safe): How probable is it not safe to advance \n",
    "                             forward?\n",
    "        3. Node P(not sure): How probable is it to NOT have enough \n",
    "                             information to take a decision?\n",
    "        \n",
    "        In this case, the good thing is that it can learn both the \n",
    "        concepts of 'Safe' and 'Unsafe' situations, with some \n",
    "        leverage for new situations without enough prior \n",
    "        information to take an informed decision. That is, it \n",
    "        could evaluate \n",
    "        \n",
    "        This could be good for a classification task.  It might be \n",
    "        possible to make the decision based on both informations \n",
    "        (if ( P(Safe) > 0.99 & P() ) )\n",
    "        But if a \n",
    "        decision must be taken, then there is no leverage for an \n",
    "        intermediate state (\"not sure\").\n",
    "\n",
    "Easy simulation: Take only half of MNIST classes (e.g. from 0 to 4) to train a classifier, then classify a test set with all 10 classes. \n",
    "    1- Few samples not in train dataset\n",
    "    2- Proportional number of images from new/known classes\n",
    "    3- Higher proportion of number of images from new classes than \n",
    "    known classes\n",
    "\n",
    "Could semi-supervised help?\n",
    "Could it also make it more robust to WRONG LABELING (easy experiment where higher proportions of labels are replaced randomly, or even the classes with higher probabilities of being misclassified would be increased)?\n",
    "    -This could be useful if input has chances of being corrupted \n",
    "     or high chance of misclassification \n",
    "     e.g. Psychiatric disorders, such as Major Depressive Disorder \n",
    "     (MDD). In that case, MDD is defined by a phenotypic \n",
    "     description of a PROBABLE disorder. MDD has a very confusing \n",
    "     and \"floating\" definition. More and more subclasses are used\n",
    "     to justify cases that don't fit, with very specific and \n",
    "     uncommon subcategories. All these definitions assume that \n",
    "     depression is a single disease which is hard to define. \n",
    "     My hypothesis is that MDD's definition was created to englobe \n",
    "     all those people that were unnaturally melancholic, which was \n",
    "     seen has a disease of the soul (CONFIRM THIS ANECDOTE OR \n",
    "     REMOVE). It is such an old concept that a seemingly robust \n",
    "     definition was developped to \"diagnose\" this disorder. In the \n",
    "     end, depression is up to interpretation and anyone might be \n",
    "     influenced by there \"gut feeling\", or the people might have \n",
    "     provided erroneous informations (misleadingly or not); no one \n",
    "     know for sure how there mood can compare to someone else's, \n",
    "     has no one has ever been anyone else (on a scientific point of \n",
    "     view at least). The data should be analysed with this in mind.\n",
    "     \n",
    "    -The unsure option should make the other definitions better; \n",
    "     only the samples that are likely enough will be used to define \n",
    "     the prior distributions for each category.\n",
    "     \n",
    "\n",
    "2- The human accuracy on a dataset of images is not perfect (some deep nets are better than a rough estimation of the accuracy of a human*). If we are asked what something is, if we absolutely have to chose a category, we will choose one, even if it is not exactly what we think it is. \n",
    "\n",
    "I hypothesize that, if given an additional choice (\"Not applicable\" or \"None of the choices\"), humans might have chosen this choice on images that are not clear. Maybe some things can suggest a correct classification which would not be picked up by a human, but there is also the possibility that some images might be corrupting the dataset.\n",
    "Possible cases of corruption:\n",
    "    An image has the wrong label\n",
    "        Typo\n",
    "        The person comprehension of a concept is\n",
    "            biased\n",
    "            incomplete\n",
    "            Naive\n",
    "            Wrong ()\n",
    "    The concept behind the classification is not correctly defined\n",
    "        **Many concepts could \n",
    "\n",
    "* I refer to the imagenet dataset. In a class, the teacher Aaron Courville explained that a single person manually classified all the images in the dataset and got approx. 95% accuracy, which some deep nets can do better. This is by no mean a proof that the deep nets are better at the task than humans, but just to give a comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/321841/what-are-variational-autoencoders-and-to-what-learning-tasks-are-they-used\n",
    "\n",
    "Ideal parameters:\n",
    "$$ \\min_{\\boldsymbol{\\lambda}}\\mathcal{D}[p(\\mathbf{z}\\vert \\mathbf{x})\\vert\\vert q(\\mathbf{z}\\vert \\mathbf{x},\\boldsymbol{\\lambda})] $$\n",
    "\n",
    "It should also minimize the reconstruction loss (and optional regularization terms, mainly L1 or L2)\n",
    "\n",
    "VAE loss function:\n",
    "$$ELBO(\\boldsymbol{\\lambda})= E_{q(\\boldsymbol{z}\\vert \\mathbf{x},\\boldsymbol{\\lambda})}[\\log p(\\mathbf{x}\\vert\\boldsymbol{z})]-\\mathcal{D}[(q(\\boldsymbol{z}\\vert \\mathbf{x},\\boldsymbol{\\lambda})\\vert\\vert p(\\boldsymbol{z})]$$\n",
    "\n",
    "$$ q(\\mathbf{z}\\vert \\mathbf{x},\\boldsymbol{\\lambda}) = \\mathcal{N}(\\mathbf{z}\\vert\\boldsymbol{\\mu}(\\mathbf{x}), \\boldsymbol{\\sigma}^2(\\mathbf{x})I) $$\n",
    "\n",
    "conditional distribution:\n",
    "$$ p_{\\boldsymbol{\\phi}}(\\mathbf{x}\\vert\\mathbf{z}) = \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}(\\mathbf{z}; \\boldsymbol{\\phi}), \\boldsymbol{\\sigma}(\\mathbf{z}; \\boldsymbol{\\phi})^2I)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ ELBO(\\boldsymbol{\\theta},\\boldsymbol{\\phi})= \\sum_i E_{q_{\\boldsymbol{\\theta}}(\\boldsymbol{z}\\vert \\mathbf{x}_i,\\boldsymbol{\\lambda})}[\\log p_{\\boldsymbol{\\phi}}(\\mathbf{x}_i\\vert\\boldsymbol{z})]-\\mathcal{D}[(q_{\\boldsymbol{\\theta}}(\\boldsymbol{z}\\vert \\mathbf{x}_i,\\boldsymbol{\\lambda})\\vert\\vert p(\\boldsymbol{z})] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "os.chdir(\"../..\")\n",
    "from data_preparation.GeoParser import GeoParser\n",
    "from dimension_reduction.ordination import ordination2d\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from models.semi_supervised.deep_generative_models.models.auxiliary_dgm import AuxiliaryDeepGenerativeModel\n",
    "#from models.semi_supervised.deep_generative_models.models.ladder_dgm import LadderDeepGenerativeModel\n",
    "from models.semi_supervised.deep_generative_models.models.dgm import DeepGenerativeModel\n",
    "from utils.utils import dict_of_int_highest_elements, plot_evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# files_destinations\n",
    "home_path = \"/home/simon/\"\n",
    "destination_folder = \"annleukemia\"\n",
    "data_folder = \"data\"\n",
    "results_folder = \"results\"\n",
    "meta_destination_folder = \"pandas_meta_df\"\n",
    "\n",
    "plots_folder_path = \"/\".join([home_path, destination_folder, results_folder, \"plots/\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dataset_name = \"gse33000_and_GSE24335_GSE44768_GSE44771_GSE44770\"\n",
    "dataset_name = \"mnist\"\n",
    "activation = \"relu\"\n",
    "#nrep = 3\n",
    "betas=(0.9, 0.999)\n",
    "vae_flavour = \"o-sylvester\"\n",
    "early_stopping = 200\n",
    "labels_per_class = 1000\n",
    "n_epochs = 1000\n",
    "warmup = 100\n",
    "gt_input = 10000\n",
    "\n",
    "# if ladder is yes builds a ladder vae. Do not combine with auxiliary (yet; might be possible and relatively \n",
    "# not too hard to implement, but might be overkill. Might be interesting too)\n",
    "translate = \"n\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Types of deep generative model\n",
    "\n",
    "# Convolution neural network (convolutional VAE and convolutional classifier)\n",
    "use_conv = False #Not applicable if not sequence (images, videos, sentences, DNA...)\n",
    "\n",
    "# Ladder VAE (L-VAE)\n",
    "ladder = False\n",
    "\n",
    "# Auxiliary Variational Auto-Encoder (A-VAE)\n",
    "auxiliary = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load pre-computed vae (unsupervised learning)\n",
    "load_vae = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "l1 = 0.\n",
    "l2 = 0.\n",
    "batch_size = 128\n",
    "mc = 1 # seems to be a problem when mc > 1 for display only, results seem good\n",
    "iw = 1 # seems to be a problem when iw > 1 for display only, results seem good\n",
    "\n",
    "# Neurons layers\n",
    "a_dim = 50\n",
    "h_dims_classifier = [256]\n",
    "h_dims = [128, 64]\n",
    "z_dims = [50]\n",
    "\n",
    "# number of flows\n",
    "n_combinations = 20 #could be just 1 with number_of_flows?\n",
    "number_of_flows = 4\n",
    "num_elements = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Files destinations\n",
    "load_from_disk = True\n",
    "load_merge = False\n",
    "home_path = \"/home/simon/\"\n",
    "destination_folder = \"annleukemia\"\n",
    "data_folder = \"data\"\n",
    "results_folder = \"results\"\n",
    "meta_destination_folder = \"pandas_meta_df\"\n",
    "plots_folder_path = \"/\".join([home_path, destination_folder, \n",
    "                              results_folder, \"plots/\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_dim (making sure it stays ok for ssl_vae) 0\n",
      "self.a_dim 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/anaconda3/envs/annleukemia/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "if auxiliary:\n",
    "    dgm = AuxiliaryDeepGenerativeModel(vae_flavour, z_dims, h_dims, n_flows=number_of_flows,a_dim=a_dim,\n",
    "                                       num_elements=num_elements, use_conv=False, is_hebb_layers=True,\n",
    "                                       gt_input=gt_input)\n",
    "\n",
    "    dgm.set_configs(home_path=home_path, results_folder=results_folder, data_folder=data_folder,\n",
    "                    destination_folder=destination_folder, dataset_name=dataset_name, lr=lr,\n",
    "                    meta_destination_folder=\"meta_pandas_dataframes\", csv_filename=\"csv_loggers\", \n",
    "                    is_unlabelled=True)\n",
    "\n",
    "elif ladder:\n",
    "    dgm = LadderDeepGenerativeModel(vae_flavour, z_dims, h_dims, n_flows=number_of_flows, auxiliary=False,\n",
    "                                    is_hebb_layers=True, gt_input=gt_input)\n",
    "\n",
    "    dgm.set_configs(home_path=home_path, results_folder=results_folder, data_folder=data_folder,\n",
    "                    destination_folder=destination_folder, dataset_name=dataset_name, lr=lr,\n",
    "                    meta_destination_folder=\"meta_pandas_dataframes\", csv_filename=\"csv_loggers\", \n",
    "                    is_unlabelled=True)\n",
    "else:\n",
    "    dgm = DeepGenerativeModel(vae_flavour, z_dims, h_dims, n_flows=number_of_flows, a_dim=0, auxiliary=False,\n",
    "                              num_elements=num_elements, is_hebb_layers=False, gt_input=gt_input)\n",
    "\n",
    "    dgm.set_configs(home_path=home_path, results_folder=results_folder, data_folder=data_folder,\n",
    "                    destination_folder=destination_folder, dataset_name=dataset_name, lr=lr,\n",
    "                    meta_destination_folder=\"meta_pandas_dataframes\", csv_filename=\"csv_loggers\", \n",
    "                    is_unlabelled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.train_loader 468\n",
      "self.train_loader 128\n",
      "self.train_loader <torch.utils.data.sampler.RandomSampler object at 0x7f87a562bb70>\n"
     ]
    }
   ],
   "source": [
    "dgm.load_example_dataset(dataset=\"mnist\", batch_size=batch_size, \n",
    "                         labels_per_class=labels_per_class, \n",
    "                         extra_class=True, unlabelled_train_ds=True, normalize=True, mu=0.1307, var=0.3081)\n",
    "\n",
    "is_example = True\n",
    "# GET ordination from this!\n",
    "train = np.vstack([x[0].data.numpy() for x in dgm.x_train])\n",
    "#unlabelled_train = np.vstack([x[0].data.numpy() for x in dgm.unlabelled_x_train])\n",
    "\n",
    "targets = np.vstack([x[1].data.numpy() for x in dgm.x_train])\n",
    "labels = [x.tolist().index(1) for x in targets]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "meta_df = pd.DataFrame(train.transpose(), columns=labels)\n",
    "#unlabeled_meta_df = pd.DataFrame(unlabelled_train.transpose())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ordination2d(meta_df, epoch=\"pre\", dataset_name=dataset_name, ord_type=\"pca\",\n",
    "#             images_folder_path=plots_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ordination2d(meta_df, epoch=\"pre\", dataset_name=dataset_name, ord_type=\"lda\",\n",
    "#             images_folder_path=plots_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subset = pd.DataFrame(train[0:1000].transpose(), columns=labels[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tSNE\n",
    "#ordination2d(subset, epoch=\"pre\", dataset_name=dataset_name, ord_type=\"tsne\", \n",
    "#             images_folder_path=plots_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape (784, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape\", meta_df.shape)\n",
    "#print(\"unlabelled meta_df shape\", unlabeled_meta_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#if meta_df is not None:\n",
    "#    dgm.import_dataframe(meta_df, batch_size, labelled=True)\n",
    "    #dgm.import_dataframe(unlabelled_meta_df, batch_size, labelled=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dgm.define_configurations(early_stopping=early_stopping, warmup=warmup, flavour=vae_flavour)\n",
    "dgm.set_data(labels_per_class=labels_per_class, is_example=True, extra_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isotropic gaussian\n",
      "self.input_size 784\n",
      "self.input_size 784\n",
      "Warmup on:  187200\n",
      "Log file created:  logs/AuxiliaryDeepGenerativeModel_parameters.log\n",
      "Log file created:  logs/AuxiliaryDeepGenerativeModel_involvment.log\n",
      "Log file created:  logs/AuxiliaryDeepGenerativeModel.log\n",
      "Labeled shape 468\n",
      "Unlabeled shape 468\n",
      "epoch 0\n",
      "Progress: 0.21%"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [128 x 784], m2: [795 x 128] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9f8313e856a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m dgm.run(n_epochs, auxiliary, mc, iw, lambda1=l1, lambda2=l2, verbose=2, \n\u001b[1;32m     26\u001b[0m         \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_pca_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_lda_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_pca_generated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         is_input_pruning=True, start_pruning=3, show_lda_generated=10)\n\u001b[0m",
      "\u001b[0;32m~/annleukemia/models/semi_supervised/deep_generative_models/models/dgm.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, n_epochs, auxiliary, mc, iw, lambda1, lambda2, verbose, show_progress, show_pca_train, show_lda_train, show_pca_valid, show_pca_generated, clip_grad, warmup_n, is_input_pruning, start_pruning, schedule, show_lda_generated, is_balanced_relu, limit_examples, keep_history, decay, alpha_rate, t_max, generate_extra_class)\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                 \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_bool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m                 \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_bool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_bool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/annleukemia/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/annleukemia/models/semi_supervised/deep_generative_models/inference/variational.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, valid_bool, valid)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mreconstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sylvester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauxiliary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mreconstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sylvester_dgm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauxiliary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mlikelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/annleukemia/models/semi_supervised/deep_generative_models/models/auxiliary_dgm.py\u001b[0m in \u001b[0;36mrun_sylvester_dgm\u001b[0;34m(self, x, auxiliary, k, y)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mauxiliary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_a_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_a_log_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_det_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sylvester\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauxiliary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0ma_kl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/annleukemia/models/generative/autoencoders/vae/sylvester_vae.py\u001b[0m in \u001b[0;36mrun_sylvester\u001b[0;34m(self, x, y, a, k, auxiliary)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_det_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mz_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauxiliary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauxiliary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0;31m# Orthogonalize all q matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"o-sylvester\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/annleukemia/models/generative/autoencoders/vae/sylvester_vae.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x, y, a, auxiliary, i)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mauxiliary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             (z_q, mean_z, var_z), h = self.aux_encoder(x, y=torch.FloatTensor([]).cuda(), a=torch.FloatTensor([]).cuda(),\n\u001b[0;32m--> 297\u001b[0;31m                                                        input_shape=self.input_shape)\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/annleukemia/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/annleukemia/models/generative/autoencoders/vae/vae.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y, a, input_shape)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgates_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mx_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgate_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/annleukemia/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/annleukemia/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/annleukemia/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [128 x 784], m2: [795 x 128] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249"
     ]
    }
   ],
   "source": [
    "if auxiliary:\n",
    "    if use_conv:\n",
    "        dgm.set_conv_adgm_layers()\n",
    "    else:\n",
    "        dgm.set_adgm_layers(h_dims=h_dims_classifier, input_shape=[1, 28, 28])\n",
    "elif ladder:\n",
    "    dgm.set_ldgm_layers(hebb_layers=True, n_channels=1)\n",
    "else:\n",
    "    if use_conv:\n",
    "        dgm.set_conv_dgm_layers(hebb_layers=True)\n",
    "    else:\n",
    "        print(\"MAIN DGM NS\")\n",
    "        dgm.set_dgm_layers(hebb_layers=True)\n",
    "\n",
    "# import the M1 in the M1+M2 model (Kingma et al, 2014). Not sure if it still works... \n",
    "if load_vae:\n",
    "    print(\"Importing the model: \", dgm.model_file_name)\n",
    "    if use_conv:\n",
    "        dgm.import_cvae()\n",
    "    else:\n",
    "        dgm.load_model()\n",
    "    #dgm.set_dgm_layers_pretrained()\n",
    "dgm.cuda()\n",
    "# dgm.vae.generate_random(False, batch_size, z1_size, [1, 28, 28])\n",
    "dgm.run(n_epochs, auxiliary, mc, iw, lambda1=l1, lambda2=l2, verbose=2, \n",
    "        show_progress=10, show_pca_train=10, show_lda_train=10, show_pca_generated=10, clip_grad=0, \n",
    "        is_input_pruning=True, start_pruning=3, show_lda_generated=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "Auto-Encoding Variational Bayes https://arxiv.org/abs/1312.6114\n",
    "Semi-Supervised Learning with Deep Generative Models https://arxiv.org/abs/1406.5298\n",
    "Ladder Variational Autoencoders https://arxiv.org/abs/1602.02282\n",
    "Auxiliary Deep Generative Models    https://arxiv.org/abs/1602.05473\n",
    "Sylvester Normalizing Flows for Variational Inference  https://arxiv.org/abs/1803.05649\n",
    "Improving Variational Auto-Encoders using Householder Flow https://arxiv.org/abs/1611.09630\n",
    "Variational Inference with Normalizing Flows https://arxiv.org/abs/1505.05770\n",
    "Convex combination linear IAF and the Householder Flow, J.M. Tomczak & M. Welling https://jmtomczak.github.io/deebmed.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Github Ressources:\n",
    "    https://github.com/wohlert/semi-supervised-pytorch\n",
    "    https://github.com/jmtomczak/vae_vpflows\n",
    "    https://github.com/jmtomczak/vae_householder_flow"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "    Calculate negative log-likelihood (to be maximized)\n",
    "    Get ordinations for all_samples AND generated samples in the SAME plot; \n",
    "        (make them the same color, but only the genwerated with a black contour, \n",
    "        or the opposite, or different contours)\n",
    "    Add Inverse Autoregressive Flow for variational inference (IAF; very common and apparently good; \n",
    "        the base for other flavours, such as sylvester flows (to be verified)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
